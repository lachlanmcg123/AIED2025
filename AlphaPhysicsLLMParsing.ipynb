{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb32caa3-8e4f-4a29-b026-48ddad44bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sympy\n",
    "from sympy import symbols, Abs\n",
    "import re\n",
    "import ollama\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "###70B parameter models\n",
    "#ollama run llama3.3\n",
    "#ollama run llama3.1:70b - this is lame\n",
    "#ollama run deepseek-r1:70b\n",
    "\n",
    "###14B Parameter Models\n",
    "#ollama run phi4:14b-fp16\n",
    "#ollama run deepseek-r1:14b-qwen-distill-fp16\n",
    "\n",
    "###14B Parameter Models 8-bit quantised\n",
    "#ollama run deepseek-r1:14b-qwen-distill-q8_0 \n",
    "#ollama run phi4:14b-q8_0\n",
    "\n",
    "#7B Parameter\n",
    "#ollama run mathstral:7b-v0.1-fp16\n",
    "#ollama run deepseek-r1:7b-qwen-distill-fp16\n",
    "\n",
    "#3B Parameter Models 16-bit quantised\n",
    "#ollama run llama3.2:3b-instruct-fp16\n",
    "#ollama run gemma2:2b-instruct-fp16\n",
    "\n",
    "#1B Parameter Models 16 bit quantised\n",
    "#ollama run llama3.2:1b-instruct-fp16\n",
    "#ollama run deepseek-r1:1.5b-qwen-distill-fp16\n",
    "\n",
    "###For Debugging\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID_1 = \"phi4\"\n",
    "MODEL_ID_2 = \"phi4:14b-q8_0\"\n",
    "MODEL_IDS = [MODEL_ID_1, MODEL_ID_2]\n",
    "\n",
    "# Configuration variables\n",
    "FILE_DIRECTORY = r\"D:\\AI_Marking\\Datasets\\\" \n",
    "#Replace above with actual directory\n",
    "ORIGINAL_FILE = os.path.join(FILE_DIRECTORY, r\"Filename.txt\") #Replace with actual filename\n",
    "model_filename = \"_\".join(MODEL_IDS)\n",
    "model_filename = re.sub(r'[\\\\/:*?\"<>|]', '-', model_filename)\n",
    "PROCESSED_FILE = os.path.join(FILE_DIRECTORY, f\"QuestionNumber{model_filename}.txt\") #Change output filename as desired\n",
    "DICTIONARY_DIRECTORY = r\"D:\\AI_Marking\\Datasets\\Cache\"\n",
    "\n",
    "# Expected variables in student responses\n",
    "EXPECTED_VARIABLES = [\"v\", \"S\", \"w\", \"u\", \"t\"] #Ensure that the variable being solved for is the first entry in the list (if relevant)\n",
    "\n",
    "# Marker list for response parsing\n",
    "MARKER_LIST = [\n",
    "    \"List of Equations: \"\n",
    "] #If you wish to achieve multiple tasks in one LLM call then you need to use the marker list to capture each part. Otherwise this should just match what you use in your LLM prompt\n",
    "\n",
    "Default_Response = (\n",
    "    \"List of Equations: [] \"\n",
    ")#If the student response is blank, then this output is given instead of calling the LLM. \n",
    "\n",
    "# Column names for DataFrame\n",
    "def marker_to_base_name(marker: str) -> str:\n",
    "    #Convert the marker text into a usable column name (lowercase, underscores, etc.).\n",
    "    #E.g. \"List of Equations: \" -> \"list_of_equations\"\n",
    "    #     \"Student notes v-u limit: \" -> \"student_notes_v_u_limit\"\n",
    "    \n",
    "    # Remove trailing colon and extra spaces\n",
    "    text = marker.replace(\":\", \"\").strip()\n",
    "    # Replace hyphens with underscores first\n",
    "    text = text.replace(\"-\", \"_\")\n",
    "    # Replace spaces with underscores\n",
    "    text = text.replace(\" \", \"_\").lower()\n",
    "    return text\n",
    "base_column_names = [marker_to_base_name(m) for m in MARKER_LIST]\n",
    "\n",
    "COLUMN_NAMES = []\n",
    "for bc in base_column_names:\n",
    "    COLUMN_NAMES.append(f\"Model1_{bc}\")\n",
    "    COLUMN_NAMES.append(f\"Model2_{bc}\")\n",
    "\n",
    "MODEL_ERROR_COLUMNS = [\n",
    "    'Model1_Batch1_Fails',\n",
    "    'Model2_Batch2_Fails',\n",
    "    'Model1_Consensus_Fails',\n",
    "    'Model2_Consensus_Fails'\n",
    "]\n",
    "\n",
    "TIME_TRACKING_COLUMNS = [\n",
    "    'Model1_Raw_Time', 'Model2_Raw_Time',\n",
    "    'Model1_Batch1_Time', 'Model2_Batch2_Time',\n",
    "    'Consensus_Time',\n",
    "    'Total_Processing_Time'\n",
    "]\n",
    "\n",
    "TOKEN_TRACKING_COLUMNS = [\n",
    "        'Model1_Raw_Tokens', 'Model2_Raw_Tokens',\n",
    "        'Model1_Batch1_Tokens', 'Model2_Batch2_Tokens',\n",
    "        'Consensus_Tokens'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f210f-b851-4cbf-b424-91248bfad6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49442aff-ce5e-4a3f-ab82-0b8d3f78174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Class and Functions\n",
    "def sanitize_filename(filename):\n",
    "    # Replace characters that are invalid in Windows filenames\n",
    "    return re.sub(r'[\\\\/:*?\"<>|]', '-', filename)\n",
    "\n",
    "class LLMResponseValidator:\n",
    "    def __init__(self, max_repair_attempts: int = 3, verbose: bool = False):\n",
    "        #Initialize the validator with configuration settings.\n",
    "        #Args:\n",
    "        #    max_repair_attempts: Maximum number of repair attempts for parsing/consensus\n",
    "\n",
    "        self.model_ids = MODEL_IDS\n",
    "        self.max_repair_attempts = max_repair_attempts\n",
    "        self.verbose = verbose\n",
    "        self.variables = symbols(' '.join(EXPECTED_VARIABLES))\n",
    "        \n",
    "        # Error and attempt tracking\n",
    "        self.current_error = None  # Most recent error message\n",
    "        self.parse_error_count = 0  # Count of syntax/parsing errors\n",
    "        self.mismatch_error_count = 0  # Count of mathematical mismatches between models\n",
    "        self.current_attempt = 0  # Current attempt number for repair/consensus\n",
    "        self.error_history = {\n",
    "            'parse_errors': [],  # History of parsing errors\n",
    "            'mismatch_errors': []  # History of mathematical mismatch errors\n",
    "        }\n",
    "        self.model1_fails = 0\n",
    "        self.model2_fails = 0\n",
    "        self.model1_batch_time = 0\n",
    "        self.model2_batch_time = 0\n",
    "        self.model1_raw_time = 0.0\n",
    "        self.model2_raw_time = 0.0\n",
    "        self.consensus_time = 0\n",
    "        self.total_processing_time = 0\n",
    "\n",
    "        self.model1_raw_tokens = 0\n",
    "        self.model2_raw_tokens = 0\n",
    "        self.model1_batch1_tokens = 0\n",
    "        self.model2_batch2_tokens = 0\n",
    "        self.consensus_tokens = 0\n",
    "        \n",
    "        # Set up logging (keep for debugging purposes)\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            filename='llm_validation.log'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset all state tracking variables for new processing attempt\"\"\"\n",
    "        self.current_error = None\n",
    "        self.parse_error_count = 0\n",
    "        self.mismatch_error_count = 0\n",
    "        self.current_attempt = 0\n",
    "        self.error_history = {\n",
    "            'parse_errors': [],\n",
    "            'mismatch_errors': []\n",
    "        }\n",
    "\n",
    "    def parse_equation(self, equation_str: str, model_label: str) -> Optional[Union[sympy.Expr, List[sympy.Expr]]]:\n",
    "        #Parse equation string into SymPy expression(s).\n",
    "        try:\n",
    "            # Handle empty or invalid input\n",
    "            if not equation_str: \n",
    "                if model_label == \"Model1\":\n",
    "                    self.model1_fails +=1\n",
    "                elif model_label == \"Model2\":\n",
    "                    self.model2_fails +=1\n",
    "                else:\n",
    "                    print(f\"Model Label was {model_label}; neither Model1 or Model2\")\n",
    "                return None\n",
    "\n",
    "            if equation_str == '[]':\n",
    "                return 0\n",
    "        \n",
    "            # Clean the equation string\n",
    "            equation_str = equation_str.strip('[]')\n",
    "            if '=' in equation_str:\n",
    "                # Split multiple equations if present\n",
    "                equations = [eq.strip() for eq in equation_str.split(',')]\n",
    "                results = []\n",
    "                for eq in equations:\n",
    "                    if '=' in eq:\n",
    "                        eq = eq.split('=')[1].strip()\n",
    "                    eq = eq.lower()\n",
    "                    results.append(sympy.sympify(eq)) #Turns strings into Sympy equations/expressions\n",
    "                return results if len(results) > 1 else results[0]\n",
    "            \n",
    "            # Convert to lowercase and parse\n",
    "            equation_str = equation_str.lower()\n",
    "            result = sympy.sympify(equation_str)\n",
    "            self.current_error = None  # Clear error on successful parse\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error parsing equation '{equation_str}': {str(e)}\"\n",
    "            self.current_error = error_msg\n",
    "            self.parse_error_count += 1\n",
    "            self.error_history['parse_errors'].append(error_msg)\n",
    "            self.logger.error(error_msg)\n",
    "            if model_label == \"Model1\":\n",
    "                self.model1_fails +=1\n",
    "            elif model_label == \"Model2\":\n",
    "                self.model2_fails +=1\n",
    "            else:\n",
    "                print(f\"Model Label was {model_label}; neither Model1 or Model2\")\n",
    "            return None\n",
    "\n",
    "    def equations_match(self, eq1: str, eq2: str) -> Tuple[bool, List[Tuple[int, int]]]:\n",
    "        #Check if two equation strings are mathematically equivalent.\n",
    "        #Returns:\n",
    "        #    Tuple[bool, List[Tuple[int, int]]]: \n",
    "        #        - Boolean indicating if all equations match\n",
    "        #        - List of matching equation indices (idx1, idx2)\n",
    "\n",
    "        parsed1 = self.parse_equation(eq1, \"Model1\")\n",
    "        parsed2 = self.parse_equation(eq2, \"Model2\")\n",
    "        \n",
    "        if parsed1 is None or parsed2 is None:\n",
    "            return False, []\n",
    "            \n",
    "        try:\n",
    "            # Convert single equations to lists for consistent handling\n",
    "            eqs1 = parsed1 if isinstance(parsed1, list) else [parsed1]\n",
    "            eqs2 = parsed2 if isinstance(parsed2, list) else [parsed2]\n",
    "            \n",
    "            # Keep track of which equations match\n",
    "            matches = []\n",
    "            used_indices = set()\n",
    "            \n",
    "            # Try to find matches for each equation in eqs1\n",
    "            for i, e1 in enumerate(eqs1):\n",
    "                for j, e2 in enumerate(eqs2):\n",
    "                    if j in used_indices:\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        # Try direct comparison\n",
    "                        diff = sympy.simplify(e1 - e2)\n",
    "                        if diff == 0:\n",
    "                            matches.append((i, j))\n",
    "                            used_indices.add(j)\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        # If we can't subtract, try string comparison\n",
    "                        if str(e1) == str(e2):\n",
    "                            matches.append((i, j))\n",
    "                            used_indices.add(j)\n",
    "                            break\n",
    "            \n",
    "            # Print debug info about matches\n",
    "            if self.verbose:\n",
    "                print(f\"\\nFound {len(matches)} matching equations:\")\n",
    "                for i, j in matches:\n",
    "                    print(f\"Equation {i} from first set matches equation {j} from second set\")\n",
    "                if len(matches) < max(len(eqs1), len(eqs2)):\n",
    "                    print(\"Some equations did not match:\")\n",
    "                    for i, e1 in enumerate(eqs1):\n",
    "                        if i not in [m[0] for m in matches]:\n",
    "                            print(f\"Unmatched from first set: {e1}\")\n",
    "                    for j, e2 in enumerate(eqs2):\n",
    "                        if j not in [m[1] for m in matches]:\n",
    "                            print(f\"Unmatched from second set: {e2}\")\n",
    "            \n",
    "            # All equations match if we found matches for all equations in both sets\n",
    "            all_match = (len(matches) == len(eqs1) == len(eqs2))\n",
    "            \n",
    "            return all_match, matches\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error comparing equations: {str(e)}\"\n",
    "            self.mismatch_error_count += 1\n",
    "            self.error_history['mismatch_errors'].append(error_msg)\n",
    "            self.logger.error(error_msg)\n",
    "            return False\n",
    "\n",
    "    def create_repair_prompt(self, original_response: str, error_msg: str, \n",
    "                           student_answer: str) -> str:\n",
    "        #Create prompt for repairing invalid responses.\n",
    "        return f\"\"\"You are an expert in interpreting student mathematical responses and converting them into valid SymPy syntax. \n",
    "        Your overall goal is to extract all equations that students have explicitly written in their response to a physics question. \n",
    "        Note that equations described in words do not count, only record equations that students have written in SymPy syntax.\n",
    "        The student may have made syntactic errors, please use your expertise to interpret what the student meant and write record the syntactically correct version.\n",
    "        Ensure that only the expected variables are used, they are listed here: {EXPECTED_VARIABLES}\n",
    "\n",
    "        The student's answer is as follows: <<<{student_answer}>>>\n",
    "        Previously you recorded this equation: {original_response}\n",
    "        However it could not be parsed, giving this error: {error_msg}\n",
    "        \n",
    "        Your task is to repair your previous response which had a parsing error. \n",
    "\n",
    "        Guidelines for your corrected response:\n",
    "        1. Begin with \"List of Equations: [\" and end with \"]\".\n",
    "        2. All equations must be in SymPy format using Eq(left, right), e.g., \"Eq(v, u + w)\" for \"v = u + w\"\n",
    "        3. If student writes expression without \"{EXPECTED_VARIABLES[0]}=\", assume that this is intended to be the right hand side of the equation and the left side would be {EXPECTED_VARIABLES[0]}\n",
    "        4. If multiple equations, separate them with commas\n",
    "        5. Only use these variables: {EXPECTED_VARIABLES}. Capitalisation of the variables is very important, please adjust the student's variables to match the capatilisation of the expected variables given here.\n",
    "        6. Correct notation errors (capitalization, brackets) but not mathematical errors\n",
    "        7. Use \"**(1/2)\" for square root\n",
    "        8. If there are multiple equations, separate them by commas but include all of them within \"[\" and \"]\".\n",
    "        9. Example formats (pay careful attention to brackets and order of operations):\n",
    "           - Student writes \"v=2u+w\": Output should be \"Eq(v, (2*u) + w)\"\n",
    "           - Student writes \"v0=u^2/w\": Output should be \"Eq(v_0, (u**2)/w)\"\n",
    "           - Student writes \"u+w^2\": Output should be \"Eq(v, u + (w**2))\"\n",
    "        \n",
    "        Provide only the \"List of Equations: [...]\" response, with no explanation or justification.\n",
    "        \"\"\"\n",
    "\n",
    "    def create_consensus_prompt(self, student_answer: str, response1: str, \n",
    "                              response2: str, model1_id: str, model2_id: str) -> str:\n",
    "        #Create prompt for reaching consensus between different responses.\n",
    "        return f\"\"\"You are an expert in interpreting student mathematical responses and converting them into valid SymPy syntax.\n",
    "        \n",
    "        Two different interpretations were given for a student's answer, and we need your expert analysis to determine the most accurate interpretation.\n",
    "\n",
    "        Original student answer: <<<{student_answer}>>>\n",
    "        {model1_id} interpretation: {response1}\n",
    "        {model2_id} interpretation: {response2}\n",
    "\n",
    "        Guidelines for your consensus response:\n",
    "        1. Begin with \"List of Equations: [\". \n",
    "        2. Then write the equation(s) in the student response. \n",
    "        3. Finally end your response with \"]\".\n",
    "        \n",
    "        All equations must be in SymPy format using Eq(left, right).\n",
    "           - If student writes expression without \"{EXPECTED_VARIABLES[0]}=\", assume that this is intended to be the right hand side of the equation and the left side would be {EXPECTED_VARIABLES[0]}\n",
    "           - Separate multiple equations with commas\n",
    "           - Only use these variables: {EXPECTED_VARIABLES}. Capitalisation of the variables is very important, please adjust the student's variables to match the capatilisation of the expected variables given here.\n",
    "           - Correct notation (v0 → v_0, x^2 → x**2) but not math errors\n",
    "           - Use brackets for proper order of operations\n",
    "           - Use \"**(1/2)\" for square root\n",
    "           - If there are multiple equations, separate them by commas but include all of them within \"[\" and \"]\".\n",
    "           Examples:\n",
    "           - \"v=2u+w\" → \"Eq(v, (2*u) + w)\"\n",
    "           - \"v0=u^2/w\" → \"Eq(v_0, (u**2)/w)\" (Not subscripts should only be included if they are in the variables list)\n",
    "           - \"u+w^2\" → \"Eq(v, u + (w**2))\"\n",
    "\n",
    "        Analyze both interpretations and the original student answer, then provide your expert consensus in the exact format specified above. \n",
    "        Provide only the \"List of Equations: [...]\" response, with no explanation or justification.\n",
    "        \"\"\"\n",
    "\n",
    "    def build_initial_prompt(self, student_answer: str) -> str:\n",
    "        initial_prompt = f\"\"\"You are an expert in interpreting student mathematical responses to physics questions. Please read this student answer: <<<{student_answer}>>>\n",
    "\n",
    "        When writing equations, use SymPy syntax following these guidelines exactly:\n",
    "           - Use Eq(left, right) syntax, e.g., \"Eq(v, (2*u) + w)\"\n",
    "           - If student writes an expression without \"{EXPECTED_VARIABLES[0]}=\", assume that this is intended to be the right hand side of the equation and the left side would be {EXPECTED_VARIABLES[0]}\n",
    "           - Separate multiple equations with commas\n",
    "           - Only use these variables: {EXPECTED_VARIABLES}. Capitalisation of the variables is very important, please adjust the student's variables to match the capatilisation of the expected variables given here.\n",
    "           - Correct notation (for example v0 → v_0 or x^2 → x**2) but do not correct math errors\n",
    "           - Use brackets for proper order of operations\n",
    "           - Use \"**(1/2)\" for square root\n",
    "           - If there are multiple equations, separate them by commas but include all of them within \"[\" and \"]\".\n",
    "           Examples:\n",
    "           - \"v=2u+w\" → \"Eq(v, (2*u) + w)\"\n",
    "           - \"v0=u^2/w\" → \"Eq(v_0, (u**2)/w)\" (Not subscripts should only be included if they are in the variables list)\n",
    "           - \"u+w^2\" → \"Eq(v, u + (w**2))\"\n",
    "        \n",
    "        The based on these guidlines analyse the provided student answer and complete the following task exactly:\n",
    "        Write \"List of Equations: [\". Then write all equations that are contained in the student text using SymPy format following the guidelines above. Separate each equation with a comma (,). Then write \"]\".  If the student has not written an equation using symbols (only describing in words) then leave the list blank \"[]\".\n",
    "        \n",
    "        When completing the task, if the student writes any variables that are not in this list: {EXPECTED_VARIABLES} use your expert judgement to interpret and rewrite what they meant in terms of these variables.\n",
    "        Use the exact marker text and provide no additional text or justification.\n",
    "        \n",
    "        Student answer: <<<{student_answer}>>>\n",
    "        Your response to the task: \"\"\"\n",
    "        return initial_prompt\n",
    "\n",
    "    def extract_sections(self, text: str) -> Dict[str, str]:\n",
    "        #Extract sections from LLM response by finding text between markers.\n",
    "        #Returns a dictionary with snake_case keys and extracted values.\n",
    "        if pd.isna(text) or not text:\n",
    "            return {self._marker_to_key(marker): \"Error in Extraction\" \n",
    "                    for marker in MARKER_LIST}\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Process each pair of consecutive markers\n",
    "        for i in range(len(MARKER_LIST)):\n",
    "            current_marker = MARKER_LIST[i]\n",
    "            next_marker = MARKER_LIST[i + 1] if i < len(MARKER_LIST) - 1 else None\n",
    "            \n",
    "            # Find start of current section\n",
    "            start_idx = text.find(current_marker)\n",
    "            if start_idx == -1:\n",
    "                # Marker not found\n",
    "                results[self._marker_to_key(current_marker)] = \"[]\" if i == 0 else \"0\"\n",
    "                continue\n",
    "                \n",
    "            # Move index to end of marker\n",
    "            start_idx += len(current_marker)\n",
    "            \n",
    "            # Find end of section (either next marker or end of text)\n",
    "            end_idx = text.find(next_marker) if next_marker else len(text)\n",
    "            if end_idx == -1:\n",
    "                end_idx = len(text)\n",
    "                \n",
    "            # Extract and clean the value\n",
    "            value = text[start_idx:end_idx].strip()\n",
    "            \n",
    "            # Store result\n",
    "            key = self._marker_to_key(current_marker)\n",
    "            if \"equations\" in key.lower() or \"equation\" in key.lower():\n",
    "                # Keep brackets for equations (both list_of_equations and final_equation)\n",
    "                results[key] = value if value else \"[]\"\n",
    "            else:\n",
    "                # For other markers, just get the first digit or default to 0\n",
    "                results[key] = next((char for char in value if char.isdigit()), \"0\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nExtracted results:\")\n",
    "            for k, v in results.items():\n",
    "                print(f\"{k}: {v}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _marker_to_key(self, marker: str) -> str:\n",
    "        \"\"\"Convert a marker to a snake_case dictionary key.\"\"\"\n",
    "        # Remove the trailing ': ' and convert to lowercase\n",
    "        key = marker.rstrip(': ').lower()\n",
    "        # Replace hyphens with underscores\n",
    "        key = key.replace('-', '_')\n",
    "        # Replace spaces with underscores and remove special characters\n",
    "        key = re.sub(r'[^a-z0-9_\\s]', '', key)\n",
    "        key = re.sub(r'\\s+', '_', key)\n",
    "        return key\n",
    "        \n",
    "    \n",
    "    def get_llm_response(self, model_id: str, prompt: str) -> tuple[str, int]:\n",
    "        \"\"\"Get response from LLM model.\"\"\"\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nCalling {model_id}...\")\n",
    "            \n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=model_id,\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                options={\"temperature\": 0.0, \"num_predict\": 1500}\n",
    "            )\n",
    "            return response['message']['content'], response['eval_count']\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting response from {model_id}: {str(e)}\")\n",
    "            return \"\", 0\n",
    "\n",
    "\n",
    "    def reach_consensus(self, student_answer: str, equations1: str, equations2: str, \n",
    "                   df: pd.DataFrame, idx: int) -> str:\n",
    "        #Attempt to reach consensus between two model responses.\n",
    "        #Args:\n",
    "        #    student_answer: Original student answer\n",
    "        #    equations1: First model's equation response\n",
    "        #    equations2: Second model's equation response\n",
    "        #    df: DataFrame for storing results\n",
    "        #    idx: Row index in DataFrame\n",
    "        #Returns:\n",
    "        #    str: Consensus equation or appropriate fallback\n",
    "        # Step 0: Check initial parsing of both models' equations\n",
    "        current_eq1, parsed1 = self.attempt_parse(equations1)\n",
    "        current_eq2, parsed2 = self.attempt_parse(equations2)\n",
    "        #######Check, this step may be unnecessary as we are pretty sure that if it is going to successfully repair that it would have already.\n",
    "        #######We do however want the parsed1 and parsed2 variables.\n",
    "        #######Also we definitely do not want to run this before even checking if the equations are the same. This is a huge un-needed cost.\n",
    "        #####18/2/25 - introduced the new function. Note that it has many unnecessary arguments that I should remove\n",
    "        \n",
    "        # If only one parses, return that one\n",
    "        if parsed1 is not None and parsed2 is None:\n",
    "            return current_eq1\n",
    "        elif parsed2 is not None and parsed1 is None:\n",
    "            return current_eq2\n",
    "            \n",
    "        consensus_attempts = 0\n",
    "        max_consensus_attempts = 2  # Two attempts after initial\n",
    "        \n",
    "        while consensus_attempts <= max_consensus_attempts:\n",
    "            # Step 2: Check if current equations match\n",
    "            if parsed1 is not None and parsed2 is not None:\n",
    "                all_match, _ = self.equations_match(current_eq1, current_eq2)\n",
    "                if all_match:\n",
    "                    return current_eq1\n",
    "            \n",
    "            # Step 1: Check if we've exceeded max attempts\n",
    "            if consensus_attempts >= max_consensus_attempts:\n",
    "                if parsed1 is not None:\n",
    "                    return current_eq1\n",
    "                elif parsed2 is not None:\n",
    "                    return current_eq2\n",
    "                else:\n",
    "                    return \"Error: Unable to reach consensus and no valid equations available\"\n",
    "            \n",
    "            consensus_attempts += 1\n",
    "            self.logger.info(f\"Starting consensus attempt {consensus_attempts}\")\n",
    "            \n",
    "            # Steps 3-4: Get and check Model1's consensus response\n",
    "            consensus_prompt = self.create_consensus_prompt(\n",
    "                student_answer,\n",
    "                current_eq1,\n",
    "                current_eq2,\n",
    "                self.model_ids[0],\n",
    "                self.model_ids[1]\n",
    "            )\n",
    "            \n",
    "            new_eq1, tokens = self.get_llm_response(self.model_ids[0], consensus_prompt)\n",
    "            self.consensus_tokens += tokens\n",
    "            df.at[idx, 'Consensus_Tokens'] = self.consensus_tokens\n",
    "            new_eq1, parsed1 = self.attempt_parse_and_repair(\n",
    "                new_eq1, student_answer, self.model_ids[0], df, idx, \"Model1\", in_consensus_mode=True\n",
    "            )\n",
    "            if parsed1 is not None:\n",
    "                current_eq1 = new_eq1\n",
    "            \n",
    "            # Steps 5-6: Get and check Model2's consensus response\n",
    "            consensus_prompt = self.create_consensus_prompt(\n",
    "                student_answer,\n",
    "                current_eq1,\n",
    "                current_eq2,\n",
    "                self.model_ids[1],\n",
    "                self.model_ids[0]\n",
    "            )\n",
    "            \n",
    "            new_eq2, tokens = self.get_llm_response(self.model_ids[1], consensus_prompt)\n",
    "            self.consensus_tokens += tokens\n",
    "            df.at[idx, 'Consensus_Tokens'] = self.consensus_tokens\n",
    "            new_eq2, parsed2 = self.attempt_parse_and_repair(\n",
    "                new_eq2, student_answer, self.model_ids[1], df, idx, \"Model2\", in_consensus_mode=True\n",
    "            )\n",
    "            if parsed2 is not None:\n",
    "                current_eq2 = new_eq2\n",
    "                \n",
    "            # Step 7: If only one response parses, return it\n",
    "            if parsed1 is not None and parsed2 is None:\n",
    "                self.logger.info(\"Only Model1 response parsed successfully\")\n",
    "                return current_eq1\n",
    "            elif parsed2 is not None and parsed1 is None:\n",
    "                self.logger.info(\"Only Model2 response parsed successfully\")\n",
    "                return current_eq2\n",
    "                \n",
    "            self.logger.info(f\"Completed consensus attempt {consensus_attempts}\")\n",
    "            # Loop continues to check for matches if both parse\n",
    "            \n",
    "        # This should never be reached due to the checks in the loop\n",
    "        return \"Error: Unexpected end of consensus process\"\n",
    "    \n",
    "    def extract_bracket_content(self, response) -> str:\n",
    "        if not response:\n",
    "            return \"\"\n",
    "        pattern = r'\\[(.*?)\\]'  # non-greedy match of anything between [ and ]\n",
    "        match = re.search(pattern, str(response), re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return \"\"\n",
    "\n",
    "    def store_extracted_sections_in_df(self, df: pd.DataFrame, row_idx: int, model_label: str, \n",
    "                                  response_text: str, repairing_equations: bool = False, in_consensus_mode: bool = False):\n",
    "        #Stores extracted sections in the DataFrame.\n",
    "        #Args:\n",
    "        #    df: DataFrame to update\n",
    "        #    row_idx: Index of row to update\n",
    "        #    model_label: Label of model (e.g., \"Model1\" or \"Model2\")\n",
    "        #    response_text: Text response to extract sections from\n",
    "        #    repairing_equations: If True, only update equations column\n",
    "        sections_dict = self.extract_sections(response_text)\n",
    "    \n",
    "        for base_col_name, content in sections_dict.items():\n",
    "            # Skip non-equation columns if we're only repairing equations\n",
    "            if repairing_equations and \"equations\" not in base_col_name:\n",
    "                continue\n",
    "                \n",
    "            # Strip any existing model prefixes before adding the new one\n",
    "            clean_base_name = base_col_name\n",
    "            for prefix in ['Model1_', 'Model2_']:\n",
    "                if clean_base_name.startswith(prefix):\n",
    "                    clean_base_name = clean_base_name[len(prefix):]\n",
    "        \n",
    "            # Build final DF column name\n",
    "            final_col_name = f\"{model_label}_{clean_base_name}\"\n",
    "\n",
    "        \n",
    "            if \"equations\" in base_col_name:\n",
    "                # This is the line for the bracket content, e.g. \"list_of_equations\"\n",
    "                bracket_content = self.extract_bracket_content(content)\n",
    "                # Store with the enclosing brackets\n",
    "                if not in_consensus_mode:#in_consensus_mode:\n",
    "                    df.at[row_idx, final_col_name] = f\"[{bracket_content}]\"\n",
    "    \n",
    "            else:\n",
    "                # Attempt to convert \"1\"/\"0\" to int, else store string\n",
    "                try:\n",
    "                    df.at[row_idx, final_col_name] = int(content)\n",
    "                except ValueError:\n",
    "                    df.at[row_idx, final_col_name] = content\n",
    "        return df\n",
    "\n",
    "    def attempt_parse_and_repair(self, equations_str: str, student_answer: str, \n",
    "                                   model_id: str, df: pd.DataFrame, idx: int, \n",
    "                                   model_label: str, in_consensus_mode: bool = False) -> Tuple[str, Optional[Union[sympy.Expr, List[sympy.Expr]]]]:\n",
    "        #Helper function to attempt parsing and repair if needed.\n",
    "        #Args:\n",
    "        #    equations_str: Equation string to parse\n",
    "        #    student_answer: Original student answer\n",
    "        #    model_id: ID of the model to use for repairs\n",
    "        #    df: DataFrame for storing results\n",
    "        #    idx: Row index in DataFrame\n",
    "        #    model_label: Label of model (e.g., \"Model1\" or \"Model2\")\n",
    "        #Returns:\n",
    "        #    Tuple of (final equation string, parsed expression or None)\n",
    "        parsed_expr = self.parse_equation(equations_str, model_label)\n",
    "        \n",
    "        repair_attempts = 3\n",
    "        attempt_count = 0\n",
    "        while parsed_expr is None and attempt_count < repair_attempts:\n",
    "            attempt_count += 1\n",
    "            if self.error_history['parse_errors']:\n",
    "                error_msg = self.error_history['parse_errors'][-1]\n",
    "            else:\n",
    "                error_msg = \"Unknown parsing error.\"\n",
    "            \n",
    "            repair_prompt = self.create_repair_prompt(\n",
    "                original_response=equations_str,\n",
    "                error_msg=error_msg,\n",
    "                student_answer=student_answer\n",
    "            )\n",
    "            \n",
    "            repair_response, repair_tokens = self.get_llm_response(model_id, repair_prompt)\n",
    "            if model_label == \"Model1\":\n",
    "                self.model1_batch1_tokens += repair_tokens\n",
    "                #df.at[idx, 'Model1_Batch1_Tokens'] = self.model1_batch1_tokens\n",
    "            elif model_label == \"Model2\":\n",
    "                self.model2_batch2_tokens += repair_tokens\n",
    "                #df.at[idx, 'Model2_Batch2_Tokens'] = self.model2_batch2_tokens\n",
    "            self.store_extracted_sections_in_df(df, idx, model_label, repair_response, repairing_equations=True, in_consensus_mode=in_consensus_mode)\n",
    "            \n",
    "            equations_str = df.at[idx, f'{model_label}_list_of_equations']\n",
    "            parsed_expr = self.parse_equation(equations_str, model_label)\n",
    "        \n",
    "        return equations_str, parsed_expr\n",
    "\n",
    "    def attempt_parse(self, equations_str: str) -> Tuple[str, Optional[Union[sympy.Expr, List[sympy.Expr]]]]:\n",
    "        parsed_expr = self.parse_equation(equations_str, \"Consensus Check\")   \n",
    "        return equations_str, parsed_expr\n",
    "\n",
    "    \n",
    "    def load_or_create_cache(self, model_id):\n",
    "        cache_dir = DICTIONARY_DIRECTORY  # Directory to store cache files\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Sanitize model_id before using in filename\n",
    "        safe_model_id = sanitize_filename(model_id)\n",
    "        cache_file = os.path.join(cache_dir, f\"response_cache_{safe_model_id}.json\")\n",
    "        \n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except (json.JSONDecodeError, IOError) as e:\n",
    "                self.logger.error(f\"Error loading cache for {model_id}: {str(e)}\")\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def save_cache(self, cache_dict, model_id):\n",
    "        cache_dir = DICTIONARY_DIRECTORY\n",
    "        \n",
    "        # Sanitize model_id before using in filename\n",
    "        safe_model_id = sanitize_filename(model_id)\n",
    "        cache_file = os.path.join(cache_dir, f\"response_cache_{safe_model_id}.json\")\n",
    "        \n",
    "        try:\n",
    "            with open(cache_file, 'w') as f:\n",
    "                json.dump(cache_dict, f)\n",
    "        except IOError as e:\n",
    "            self.logger.error(f\"Error saving cache for {model_id}: {str(e)}\")\n",
    "    \n",
    "    def load_consensus_cache(self, filename):\n",
    "        cache_dir = DICTIONARY_DIRECTORY\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Sanitize filename before using\n",
    "        safe_filename = sanitize_filename(filename)\n",
    "        cache_file = os.path.join(cache_dir, safe_filename)\n",
    "        \n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except (json.JSONDecodeError, IOError) as e:\n",
    "                self.logger.error(f\"Error loading consensus cache: {str(e)}\")\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def save_consensus_cache(self, cache_dict, filename):\n",
    "        cache_dir = DICTIONARY_DIRECTORY\n",
    "        \n",
    "        # Sanitize filename before using\n",
    "        safe_filename = sanitize_filename(filename)\n",
    "        cache_file = os.path.join(cache_dir, safe_filename)\n",
    "        \n",
    "        try:\n",
    "            with open(cache_file, 'w') as f:\n",
    "                json.dump(cache_dict, f)\n",
    "        except IOError as e:\n",
    "            self.logger.error(f\"Error saving consensus cache: {str(e)}\")\n",
    "    \n",
    "    def process_dataframe_in_batches(self, test: bool = False, do_consensus: bool = False): \n",
    "        #Process all rows in multiple steps/batches to avoid repeated reloading of models.\n",
    "        #Saves partial progress to PROCESSED_FILE after each row to allow resuming.\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # 1. Load or create DataFrame\n",
    "        # -----------------------------------------------------------------\n",
    "        if os.path.exists(PROCESSED_FILE):\n",
    "            if self.verbose:\n",
    "                print(f\"\\nLoading existing processed file: {PROCESSED_FILE}\")\n",
    "            df = pd.read_csv(PROCESSED_FILE, sep='\\t', encoding='latin-1')\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(f\"\\nStarting new processing from: {ORIGINAL_FILE}\")\n",
    "            df = pd.read_csv(ORIGINAL_FILE, sep='\\t', encoding='latin-1')\n",
    "        \n",
    "        #model1_cache = self.load_or_create_cache(self.model_ids[0])\n",
    "        #model2_cache = self.load_or_create_cache(self.model_ids[1])\n",
    "        \n",
    "        # Read last recorded error counts\n",
    "        if os.path.exists(PROCESSED_FILE):\n",
    "            last_row_with_counts = df.loc[df['Model1_Batch1_Fails'].notna() | \n",
    "                                          df['Model2_Batch2_Fails'].notna() | \n",
    "                                          df['Model1_Consensus_Fails'].notna() | \n",
    "                                          df['Model2_Consensus_Fails'].notna()]\n",
    "            \n",
    "            if not last_row_with_counts.empty:\n",
    "                last_idx = last_row_with_counts.index.max()\n",
    "                model1_fails_val = df.at[last_idx, 'Model1_Batch1_Fails']\n",
    "                model2_fails_val = df.at[last_idx, 'Model2_Batch2_Fails']\n",
    "                \n",
    "                self.model1_fails = int(model1_fails_val) if pd.notna(model1_fails_val) else 0\n",
    "                self.model2_fails = int(model2_fails_val) if pd.notna(model2_fails_val) else 0\n",
    "\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"Resuming with Model1 fails: {self.model1_fails}, Model2 fails: {self.model2_fails}\")\n",
    "            else:\n",
    "                self.model1_fails = 0\n",
    "                self.model2_fails = 0\n",
    "        else:\n",
    "            self.model1_fails = 0\n",
    "            self.model2_fails = 0\n",
    "\n",
    "\n",
    "        # Read last recorded time values\n",
    "        if os.path.exists(PROCESSED_FILE):\n",
    "            last_row_with_times = df.loc[df['Total_Processing_Time'].notna()]\n",
    "            \n",
    "            if not last_row_with_times.empty:\n",
    "                last_idx = last_row_with_times.index.max()\n",
    "                while (last_idx in df.index and (pd.isna(df.at[last_idx, 'Model2_Batch2_Time']) or df.at[last_idx, 'Model2_Batch2_Time'] == 0.0)):\n",
    "                    last_idx -= 1\n",
    "        \n",
    "                m1_time = df.at[last_idx, 'Model1_Batch1_Time']\n",
    "                m2_time = df.at[last_idx, 'Model2_Batch2_Time']\n",
    "                cons_time = df.at[last_idx, 'Consensus_Time']\n",
    "                total_time = df.at[last_idx, 'Total_Processing_Time']\n",
    "                \n",
    "                r1_time = df.at[last_idx, 'Model1_Raw_Time']\n",
    "                if pd.notna(r1_time):\n",
    "                    self.model1_raw_time = float(r1_time)\n",
    "                \n",
    "                r2_time = df.at[last_idx, 'Model2_Raw_Time']\n",
    "                if pd.notna(r2_time):\n",
    "                    self.model2_raw_time = float(r2_time)\n",
    "                \n",
    "                self.model1_batch_time = float(m1_time) if pd.notna(m1_time) else 0.0\n",
    "                self.model2_batch_time = float(m2_time) if pd.notna(m2_time) else 0.0\n",
    "                self.consensus_time = float(cons_time) if pd.notna(cons_time) else 0.0\n",
    "                self.total_processing_time = float(total_time) if pd.notna(total_time) else 0.0\n",
    "\n",
    "                r1_raw = df.at[last_idx, 'Model1_Raw_Tokens']\n",
    "                if pd.notna(r1_raw):\n",
    "                    self.model1_raw_tokens = float(r1_raw)\n",
    "        \n",
    "                r2_raw = df.at[last_idx, 'Model2_Raw_Tokens']\n",
    "                if pd.notna(r2_raw):\n",
    "                    self.model2_raw_tokens = float(r2_raw)\n",
    "        \n",
    "                r1_b = df.at[last_idx, 'Model1_Batch1_Tokens']\n",
    "                if pd.notna(r1_b):\n",
    "                    self.model1_batch1_tokens = float(r1_b)\n",
    "        \n",
    "                r2_b = df.at[last_idx, 'Model2_Batch2_Tokens']\n",
    "                if pd.notna(r2_b):\n",
    "                    self.model2_batch2_tokens = float(r2_b)\n",
    "        \n",
    "                c_tok = df.at[last_idx, 'Consensus_Tokens']\n",
    "                if pd.notna(c_tok):\n",
    "                    self.consensus_tokens = float(c_tok)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"Resuming with cumulative times (seconds):\")\n",
    "                    print(f\"  Model1 batch: {self.model1_batch_time:.1f}\")\n",
    "                    print(f\"  Model2 batch: {self.model2_batch_time:.1f}\")\n",
    "                    print(f\"  Consensus: {self.consensus_time:.1f}\")\n",
    "                    print(f\"  Total: {self.total_processing_time:.1f}\")\n",
    "            else:\n",
    "                # Initialize time tracking variables\n",
    "                self.model1_batch_time = 0\n",
    "                self.model2_batch_time = 0\n",
    "                self.consensus_time = 0\n",
    "                self.total_processing_time = 0\n",
    "        else:\n",
    "            # Initialize time tracking variables\n",
    "            self.model1_batch_time = 0\n",
    "            self.model2_batch_time = 0\n",
    "            self.consensus_time = 0\n",
    "            self.total_processing_time = 0\n",
    "\n",
    "\n",
    "        if test:\n",
    "            df = df.head(10)\n",
    "            if self.verbose:\n",
    "                print(df)\n",
    "        \n",
    "        # Store original columns for reference\n",
    "        self.original_columns = [\n",
    "            col for col in df.columns\n",
    "            if not col.startswith(('Model1_', 'Model2_', 'Consensus_', 'Repair', 'Final_', 'Required_'))\n",
    "        ]\n",
    "    \n",
    "        # -----------------------------------------------------------------\n",
    "        # 2. Ensure required columns exist\n",
    "        # -----------------------------------------------------------------\n",
    "        needed_columns = COLUMN_NAMES + ['Consensus_Equation'] + ['Model1_raw_list_of_equations', 'Model2_raw_list_of_equations'] + MODEL_ERROR_COLUMNS + TIME_TRACKING_COLUMNS + TOKEN_TRACKING_COLUMNS\n",
    "        for col in needed_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "    \n",
    "        # -----------------------------------------------------------------\n",
    "        # 3. Batch 1: Call Model1 for all rows that need it\n",
    "        # -----------------------------------------------------------------\n",
    "        for idx, row in df.iterrows():\n",
    "            start_time = time.time()\n",
    "            model1_cache = self.load_or_create_cache(self.model_ids[0])\n",
    "            print(f\"Model1 processing for row {idx}\")\n",
    "            student_answer = str(row[self.original_columns[0]])\n",
    "            \n",
    "            if pd.isna(row['Model1_list_of_equations']) or row['Model1_list_of_equations'] in ('', 'Error in Extraction.'):\n",
    "                if not student_answer.strip() or student_answer.strip() == '-':\n",
    "                    # Handle blank answers\n",
    "                    response1 = Default_Response\n",
    "                    df.at[idx, 'Model1_raw_list_of_equations'] = f'[{self.extract_bracket_content(self.extract_sections(response1)[\"list_of_equations\"])}]'\n",
    "                    raw_elapsed = time.time() - start_time\n",
    "                    self.model1_raw_time += raw_elapsed\n",
    "                    self.model1_raw_tokens += 0\n",
    "                    self.model1_batch1_tokens += 0\n",
    "                    df.at[idx, 'Model1_Raw_Tokens'] = self.model1_raw_tokens\n",
    "                    df.at[idx, 'Model1_Raw_Time'] = self.model1_raw_time\n",
    "                    df.at[idx, 'Model1_Batch1_Tokens'] =  self.model1_batch1_tokens\n",
    "                    self.store_extracted_sections_in_df(df, idx, \"Model1\", response1)\n",
    "                else:\n",
    "                    if student_answer in model1_cache:\n",
    "                        if self.verbose:\n",
    "                            print(f\"Cache hit for Model1, row {idx}\")\n",
    "                        cached_data = model1_cache[student_answer]\n",
    "                        \n",
    "                        # Store the raw response\n",
    "                        response1 = cached_data['raw_response']\n",
    "                        df.at[idx, 'Model1_raw_list_of_equations'] = f'[{self.extract_bracket_content(self.extract_sections(response1)[\"list_of_equations\"])}]'\n",
    "                        self.model1_raw_tokens += 0\n",
    "                        self.model1_batch1_tokens += 0\n",
    "                        df.at[idx, 'Model1_Raw_Tokens'] = self.model1_raw_tokens\n",
    "                        df.at[idx, 'Model1_Batch1_Tokens'] =  self.model1_batch1_tokens\n",
    "                        raw_elapsed = time.time() - start_time\n",
    "                        self.model1_raw_time += raw_elapsed\n",
    "                        df.at[idx, 'Model1_Raw_Time'] = self.model1_raw_time\n",
    "                        self.store_extracted_sections_in_df(df, idx, \"Model1\", response1)\n",
    "                        \n",
    "                        # Store the final equations after repair\n",
    "                        if 'final_equations' in cached_data:\n",
    "                            df.at[idx, 'Model1_list_of_equations'] = cached_data['final_equations']\n",
    "                    \n",
    "                    else:\n",
    "                        # Get initial response from Model1\n",
    "                        prompt = self.build_initial_prompt(student_answer)\n",
    "                        response1, raw_tokens = self.get_llm_response(self.model_ids[0], prompt)\n",
    "                        self.model1_raw_tokens += raw_tokens\n",
    "                        self.model1_batch1_tokens += raw_tokens\n",
    "                        df.at[idx, 'Model1_Batch1_Tokens'] = self.model1_batch1_tokens\n",
    "                        df.at[idx, 'Model1_Raw_Tokens'] = self.model1_raw_tokens\n",
    "                        #print(f\"{response1=}\")\n",
    "                        df.at[idx, 'Model1_raw_list_of_equations'] = f'[{self.extract_bracket_content(self.extract_sections(response1)[\"list_of_equations\"])}]'\n",
    "                        self.store_extracted_sections_in_df(df, idx, \"Model1\", response1)\n",
    "                        raw_elapsed = time.time() - start_time\n",
    "                        \n",
    "                        self.model1_raw_time += raw_elapsed\n",
    "                        df.at[idx, 'Model1_Raw_Time'] = self.model1_raw_time\n",
    "                        \n",
    "                        # Attempt parsing and repair if needed\n",
    "                        equations_str = df.at[idx, 'Model1_list_of_equations']\n",
    "                        final_eq1, parsed1 = self.attempt_parse_and_repair(\n",
    "                            equations_str, student_answer, self.model_ids[0], df, idx, \"Model1\"\n",
    "                        )\n",
    "\n",
    "                        model1_cache[student_answer] = {\n",
    "                            'raw_response': response1,\n",
    "                            'final_equations': df.at[idx, 'Model1_list_of_equations'],\n",
    "                        }\n",
    "                        \n",
    "                        # Save the updated cache\n",
    "                        self.save_cache(model1_cache, self.model_ids[0])\n",
    "\n",
    "                # Record time and save\n",
    "                elapsed = time.time() - start_time\n",
    "                self.model1_batch_time += elapsed\n",
    "                self.total_processing_time += elapsed\n",
    "                df.at[idx, 'Model1_Batch1_Tokens'] = self.model1_batch1_tokens\n",
    "                df.at[idx, 'Model1_Batch1_Fails'] = self.model1_fails\n",
    "                df.at[idx, 'Model1_Batch1_Time'] = self.model1_batch_time\n",
    "                df.at[idx, 'Total_Processing_Time'] = self.total_processing_time\n",
    "                df.to_csv(PROCESSED_FILE, sep='\\t', index=False, encoding='utf-8')\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # 4. Batch 2: Call Model2 for all rows that need it\n",
    "        # -----------------------------------------------------------------\n",
    "        for idx, row in df.iterrows():\n",
    "            start_time = time.time()\n",
    "            # Load cache for this iteration\n",
    "            model2_cache = self.load_or_create_cache(self.model_ids[1])\n",
    "            \n",
    "            print(f\"Model2 processing for row {idx}\")\n",
    "            student_answer = str(row[self.original_columns[0]])\n",
    "            \n",
    "            if pd.isna(row['Model2_list_of_equations']) or row['Model2_list_of_equations'] in ('', 'Error in Extraction.'):\n",
    "                if not student_answer.strip() or student_answer.strip() == '-':\n",
    "                    # Handle blank answers\n",
    "                    response2 = Default_Response\n",
    "                    df.at[idx, 'Model2_raw_list_of_equations'] = f'[{self.extract_bracket_content(self.extract_sections(response2)[\"list_of_equations\"])}]'\n",
    "                    raw_elapsed = time.time() - start_time\n",
    "                    self.model2_raw_time += raw_elapsed\n",
    "                    self.model2_raw_tokens += 0\n",
    "                    self.model2_batch2_tokens += 0\n",
    "                    df.at[idx, 'Model2_Raw_Tokens'] = self.model2_raw_tokens\n",
    "                    df.at[idx, 'Model2_Raw_Time'] = self.model2_raw_time\n",
    "                    df.at[idx, 'Model2_Batch2_Tokens'] =  self.model2_batch2_tokens\n",
    "                    self.store_extracted_sections_in_df(df, idx, \"Model2\", response2)\n",
    "                else:\n",
    "                    if student_answer in model2_cache:\n",
    "                        if self.verbose:\n",
    "                            print(f\"Cache hit for Model2, row {idx}\")\n",
    "                        cached_data = model2_cache[student_answer]\n",
    "                        \n",
    "                        # Store the raw response\n",
    "                        response2 = cached_data['raw_response']\n",
    "                        df.at[idx, 'Model2_raw_list_of_equations'] = f'[{self.extract_bracket_content(self.extract_sections(response2)[\"list_of_equations\"])}]'\n",
    "                        self.model2_raw_tokens += 0\n",
    "                        self.model2_batch2_tokens += 0\n",
    "                        df.at[idx, 'Model2_Raw_Tokens'] = self.model2_raw_tokens\n",
    "                        raw_elapsed = time.time() - start_time\n",
    "                        self.model2_raw_time += raw_elapsed\n",
    "                        df.at[idx, 'Model2_Raw_Time'] = self.model2_raw_time\n",
    "                        df.at[idx, 'Model2_Batch2_Tokens'] =  self.model2_batch2_tokens\n",
    "                        self.store_extracted_sections_in_df(df, idx, \"Model2\", response2)\n",
    "                        \n",
    "                        # Store the final equations after repair\n",
    "                        if 'final_equations' in cached_data:\n",
    "                            df.at[idx, 'Model2_list_of_equations'] = cached_data['final_equations']\n",
    "                    \n",
    "                    else:\n",
    "                        # Get initial response from Model2\n",
    "                        prompt = self.build_initial_prompt(student_answer)\n",
    "                        response2, raw_tokens = self.get_llm_response(self.model_ids[1], prompt)\n",
    "                        self.model2_raw_tokens += raw_tokens\n",
    "                        self.model2_batch2_tokens += raw_tokens\n",
    "                        df.at[idx, 'Model2_Raw_Tokens'] = self.model2_raw_tokens\n",
    "                        df.at[idx, 'Model2_Batch2_Tokens'] = self.model2_batch2_tokens\n",
    "                        #print(f\"{response2=}\")\n",
    "                        df.at[idx, 'Model2_raw_list_of_equations'] = f'[{self.extract_bracket_content(self.extract_sections(response2)[\"list_of_equations\"])}]'\n",
    "                        self.store_extracted_sections_in_df(df, idx, \"Model2\", response2)\n",
    "                        raw_elapsed = time.time() - start_time\n",
    "                        \n",
    "                        self.model2_raw_time += raw_elapsed\n",
    "                        df.at[idx, 'Model2_Raw_Time'] = self.model2_raw_time\n",
    "                        \n",
    "                        # Attempt parsing and repair if needed\n",
    "                        equations_str = df.at[idx, 'Model2_list_of_equations']\n",
    "                        final_eq2, parsed2 = self.attempt_parse_and_repair(\n",
    "                            equations_str, student_answer, self.model_ids[1], df, idx, \"Model2\"\n",
    "                        )\n",
    "        \n",
    "                        model2_cache[student_answer] = {\n",
    "                            'raw_response': response2,\n",
    "                            'final_equations': df.at[idx, 'Model2_list_of_equations'],\n",
    "                        }\n",
    "                \n",
    "                        # Save the updated cache\n",
    "                        self.save_cache(model2_cache, self.model_ids[1])\n",
    "        \n",
    "                # Record time and save\n",
    "                elapsed = time.time() - start_time\n",
    "                self.model2_batch_time += elapsed\n",
    "                self.total_processing_time += elapsed\n",
    "                df.at[idx, 'Model2_Batch2_Tokens'] = self.model2_batch2_tokens\n",
    "                df.at[idx, 'Model2_Batch2_Fails'] = self.model2_fails\n",
    "                df.at[idx, 'Model2_Batch2_Time'] = self.model2_batch_time\n",
    "                df.at[idx, 'Total_Processing_Time'] = self.total_processing_time\n",
    "                df.to_csv(PROCESSED_FILE, sep='\\t', index=False, encoding='utf-8')\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # 5. Check for matches and attempt consensus where needed\n",
    "        # -----------------------------------------------------------------\n",
    "        if do_consensus:\n",
    "            for idx, row in df.iterrows():\n",
    "                start_time = time.time()\n",
    "    \n",
    "                consensus_cache_filename = f\"consensus_cache_{sanitize_filename(self.model_ids[0])}_{sanitize_filename(self.model_ids[1])}.json\"\n",
    "                consensus_cache = self.load_consensus_cache(consensus_cache_filename)\n",
    "                \n",
    "                print(f\"Consensus processing for row {idx}\")\n",
    "                student_answer = str(row[self.original_columns[0]])\n",
    "    \n",
    "                if pd.notna(row['Consensus_Equation']) and row['Consensus_Equation'] not in ('', 'Error in Extraction.'):\n",
    "                    print(f\"  Skipping row {idx} - consensus already exists\")\n",
    "                    continue\n",
    "                    \n",
    "                eq1 = row['Model1_list_of_equations'] or \"\"\n",
    "                eq2 = row['Model2_list_of_equations'] or \"\"\n",
    "        \n",
    "                # Handle empty equations\n",
    "                if not eq1.strip('[]') and not eq2.strip('[]'):\n",
    "                    df.at[idx, 'Consensus_Equation'] = '[]'\n",
    "                    elapsed = time.time() - start_time\n",
    "                    self.consensus_time += elapsed\n",
    "                    self.total_processing_time += elapsed\n",
    "                    df.at[idx, 'Consensus_Time'] = self.consensus_time \n",
    "                    df.at[idx, 'Total_Processing_Time'] = self.total_processing_time\n",
    "                    df.at[idx, 'Model1_Consensus_Fails'] = self.model1_fails\n",
    "                    df.at[idx, 'Model2_Consensus_Fails'] = self.model2_fails\n",
    "                    df.at[idx, 'Consensus_Tokens'] = self.consensus_tokens\n",
    "                    df.to_csv(PROCESSED_FILE, sep='\\t', index=False, encoding='utf-8')\n",
    "                    continue\n",
    "        \n",
    "    \n",
    "                cache_key = f\"{student_answer}\"\n",
    "                \n",
    "                if cache_key in consensus_cache:\n",
    "                    print(f\"  Consensus cache hit for row {idx}\")\n",
    "                    df.at[idx, 'Consensus_Equation'] = consensus_cache[cache_key]\n",
    "                else:\n",
    "                    # Try to reach consensus\n",
    "                    consensus_equation = self.reach_consensus(student_answer, eq1, eq2, df, idx)\n",
    "                    df.at[idx, 'Consensus_Equation'] = consensus_equation\n",
    "                    \n",
    "                    # Add to cache\n",
    "                    consensus_cache[cache_key] = consensus_equation\n",
    "                    self.save_consensus_cache(consensus_cache, consensus_cache_filename)\n",
    "    \n",
    "                # Record time and save\n",
    "                elapsed = time.time() - start_time\n",
    "                self.consensus_time += elapsed\n",
    "                self.total_processing_time += elapsed\n",
    "                df.at[idx, 'Consensus_Time'] = self.consensus_time \n",
    "                df.at[idx, 'Total_Processing_Time'] = self.total_processing_time\n",
    "                df.at[idx, 'Model1_Consensus_Fails'] = self.model1_fails\n",
    "                df.at[idx, 'Model2_Consensus_Fails'] = self.model2_fails\n",
    "                df.at[idx, 'Consensus_Tokens'] = self.consensus_tokens\n",
    "        \n",
    "                # Save progress\n",
    "                df.to_csv(PROCESSED_FILE, sep='\\t', index=False, encoding='utf-8')\n",
    "    \n",
    "        # -----------------------------------------------------------------\n",
    "        # 6. Final summary\n",
    "        # -----------------------------------------------------------------\n",
    "        if True: #self.verbose:\n",
    "            total_rows = len(df)\n",
    "            consensus_reached = df['Consensus_Equation'].notna().sum()\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"BATCH PROCESSING SUMMARY\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Total rows: {total_rows}\")\n",
    "            print(f\"Consensus reached: {consensus_reached}\")\n",
    "            print(f\"Model1 fails: {self.model1_fails}\")\n",
    "            print(f\"Model2 fails: {self.model2_fails}\")\n",
    "            print(f\"Time elapsed (seconds):\")\n",
    "            print(f\"  Model1 batch: {self.model1_batch_time:.1f}\")\n",
    "            print(f\"  Model2 batch: {self.model2_batch_time:.1f}\")\n",
    "            print(f\"  Consensus: {self.consensus_time:.1f}\")\n",
    "            print(f\"  Total: {self.total_processing_time:.1f}\")\n",
    "            print(f\"  Average per row: {self.total_processing_time/total_rows:.1f}\")\n",
    "            print(\"=\"*50)\n",
    "        \n",
    "        return df, self.model1_fails, self.model2_fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0159d-5cd6-42bf-aa68-2b1a75e2c27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d9d2b-9208-48a0-8308-29346be5ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the validator\n",
    "validator = LLMResponseValidator(max_repair_attempts=3, verbose=False)\n",
    "\n",
    "print(f\"Processing file: {ORIGINAL_FILE}\")\n",
    "print(f\"Results will be saved to: {PROCESSED_FILE}\")\n",
    "\n",
    "# Call the new batch method\n",
    "processed_df, model1fails, model2fails = validator.process_dataframe_in_batches(test = False, do_consensus = True)\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f\"Total rows processed: {len(processed_df)}\")\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nFinal Statistics:\")\n",
    "#print(f\"Total rows needing repair: {processed_df['RepairNeeded'].sum()}\")\n",
    "\n",
    "# Display first few rows of the processed dataframe\n",
    "print(\"\\nFirst few rows of processed data:\")\n",
    "display(processed_df.head())  # or just 'print(processed_df.head())'\n",
    "print(f\"{model1fails=}\")\n",
    "print(f\"{model2fails=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455c849-2809-4659-a583-40349343cbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d43a0-c6a4-4135-a20f-52481c93629d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f94091-33fb-4189-b143-77bb4b85428f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d911c43-f333-4336-8a49-bd0ea4d35f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152aad2-2250-477f-92a9-f38e9538867c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c824f-88fb-489f-a17d-c36d21e5124c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b11a48-c7bb-4023-a3b0-4bb3502c16ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982fc02-219e-4a5c-bdfb-6a217301b4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd420d68-f85a-4a0b-982c-cc591327e441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f5974-555a-4020-abc5-cf2c992262ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb5c27-686a-45fb-a034-48c5939b1092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e7c254-9ea1-4449-b312-44af6951e388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcad009-9441-4fd6-b29f-9bfcaa972ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af161e5b-3376-4c38-8299-fe72cb29e421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ab03e-b801-4c09-a90e-18d1e8fe5343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
